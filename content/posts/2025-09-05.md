---
title: "2025-09-05 Hacker News Top Articles and Its Summaries"
date: 2025-09-05T17:01:03+08:06
draft: false
tags:
  - hackernews
---

## 1. [Anthropic agrees to pay $1.5B to settle lawsuit with book authors](https://news.ycombinator.com/item?id=45142885)

**Total comment counts : 5**

### Summary

 The article instructs readers to enable JavaScript and disable ad blockers.

### Top 1 Comment Summary

 Contrary to common misperceptions, the article says the dispute isn’t about model training—training is fair use. The problem is pirating books to build training data, which allegedly occurred with Anthropic. It argues that legally obtained methods—buying used copies, scanning them, and training on them—are fine. The piece ends by noting that Rainbows End was prescient in many ways.

### Top 2 Comment Summary

 The piece questions why a large fundraising effort would be used to benefit publishers, imagining a pitch requesting billions to fund a major investment, including using the money to pay off lawsuits.

## 2. [Making a font of my handwriting](https://news.ycombinator.com/item?id=45141636)

**Total comment counts : 9**

### Summary

 He's trying to make his site feel more personal with a cursive handwriting header font. He first attempted to build a font using open-source tools—designing in Inkscape and importing to FontForge, but FontForge was painful, and Inkscape’s SVG font approach was fiddly. Open-source tutorials pointed to Calligraphr. Although wary of subscriptions, he bought a one-month option that downgrades to free afterward. He prints templates, writes letters, scans them, and the tool outputs a TTF. He selects minimal English, punctuation, and ligatures, fills four pages twice, and adds custom ligatures like Re, To, ers, ey, hy, ra, re, ty to look natural.

### Top 1 Comment Summary

 Back in 2013 I did something similar for my wedding site: a mail-in service produced a decent TTF font, which I then converted to a WOFF. The story is still online at ruthandjosh.net/story/ (cringe warning).

### Top 2 Comment Summary

 An author laments worsening handwriting and a feedback loop that makes them type instead, further eroding their handwriting through lack of practice. They suggest creating a font modeled on their handwriting, potentially usable as a cryptographic hash. They like the idea, noting that although typing feels less personal, a handwriting-based font might retain a sense of personal touch.

## 3. [Purposeful animations](https://news.ycombinator.com/item?id=45139088)

**Total comment counts : 27**

### Summary

 Animations should have a clear purpose: explain a feature, provide feedback, or delight—but only if they won’t annoy with repetition. Frequency matters: what works once may irritate if seen hundreds of times daily (e.g., Raycast, repeated tooltips). Speed matters: aim under ~300ms to feel responsive; faster spinners or shorter transitions boost perceived performance. Don’t animate for its own sake; prioritize user goals and context—sometimes the best animation is no animation at all.

### Top 1 Comment Summary

 Author argues that inconsistent UI state during animations ruins experience. Examples: Windows-style notifications where the X can’t be clicked until after the animation, causing accidental opens; Mac desktop switching where an app briefly appears before switching and slow or random animation misleads users. Takeaway: if you include animations, they must be rock-solid; otherwise users will view the designer and app as buggy or annoying.

### Top 2 Comment Summary

 Animation is often treated as polish and delight balanced against perceptual latency. The author has two critiques: 1) delight is overstated—the truly delighted are mostly designers themselves; 2) focus on state changes: if a user may miss a change, use animation to visualize it. In short, the primary purpose of animation is to convey state changes; other uses are vanity.

## 4. [MentraOS – open-source Smart glasses OS](https://news.ycombinator.com/item?id=45140381)

**Total comment counts : 12**

### Summary

 MentraOS is a smart glasses operating system with built-in apps, AI assistant, notifications, translation, screen mirroring, captions, and more. Developers write one app that runs on any Mentra-compatible glasses; the Mentra Store hosts user-ready apps. All components are open source under the MIT license, with MentraOS handling pairing, connection, data streaming, and cross-compatibility. The project promotes an open, cross-compatible, user-controlled ecosystem supported by the MentraOS Community—Discord, contributor guides, and PR-friendly development. Compatible devices include Mentra G1, Mach 1, and Live.

### Top 1 Comment Summary

 The author wonders if there are smart glasses that function only as a display. They argue that most feature sets feel like anti-features and would prefer a minimal display that can receive low-bandwidth data from a phone or another device via Bluetooth. They want to avoid built-in cameras or microphones for privacy, and view a built-in speaker as unnecessary since headphones already exist.

### Top 2 Comment Summary

 I don’t have the full article text to summarize. Please paste the article or share a link, and I’ll provide a concise summary (≤100 words). From your prompt, it seems to contrast full open-source software (all components compilable from source) with a more restricted, Espressif-style SDK-only approach.

## 5. [I ditched Docker for Podman](https://news.ycombinator.com/item?id=45137525)

**Total comment counts : 120**

### Summary

 error

### Top 1 Comment Summary

 In 2001/02, I built a minimal WiFi hotspot box using OpenBSD, aiming to slim our Python deployment, avoid copying unnecessary files, and dodge dependency-hell. I used chroot/jails, then ran the deployment code outside the jail and tracked system calls with ptrace to see which files it opened. The resulting file list was used to assemble a small, immutable deployment package. It stayed lightweight and somewhat resistant to attack. When Docker appeared, I recalled this approach and wondered if anyone had similar methods to monitor container file usage and trim unused files after observation.

### Top 2 Comment Summary

 A contrary take: Podman rocks for the author, who finds Docker hard and full of pitfalls, while Podman isn’t any worse. They also note that licensing isn’t a concern for any company they work for, calling it a win-win.

## 6. [Show HN: Open-sourcing our text-to-CAD app](https://news.ycombinator.com/item?id=45140921)

**Total comment counts : 5**

### Summary

 The article describes CADAM, a Text-to-CAD web app that uses ngrok to forward image URLs to Anthropic. Setup steps include installing ngrok, creating a tunnel to a Supabase instance, copying the generated ngrok URL, adding it to supabase/functions/.env, and setting ENVIRONMENT="local". It notes attribution and GPLv3 licensing, and page errors stating “There was an error while loading. Please reload this page.”

### Top 1 Comment Summary

 The article praises a project, notes AI performs surprisingly well with OpenSCAD, and wonders what a custom-trained model could achieve.

### Top 2 Comment Summary

 CADAM uses ngrok to relay image URLs to Anthropic. As an alternative, you can send base64-encoded PNGs directly, which removes the need for ngrok.

## 7. [All of our lives overlap in the Network Of Time](https://news.ycombinator.com/item?id=45108536)

**Total comment counts : 4**

### Summary

 It invites readers to explore the connections that bind everyone and to subscribe to the Network of Time Substack for updates and new discoveries.

### Top 1 Comment Summary

 The author ironizes that any data produced after 2023 will likely be unusable.

### Top 2 Comment Summary

 The line portrays Joe Rogan as the central hub of a “network of time,” suggesting he serves as a key connector or focal point in temporal discussions.

## 8. [The Old Robots Web Site](https://news.ycombinator.com/item?id=45098609)

**Total comment counts : 18**

### Summary

 error

### Top 1 Comment Summary

 Tip for parents of curious kids: buy small TOMY robots on eBay. They’re often broken, but their interiors make a great hands-on learning experience. TOMY toys typically run on a single DC motor, with movements, sounds, and sensing powered by gears, cams, and simple mechanisms. Repairing and reviving a 40-year-old robot teaches about simple machines and engineering.

### Top 2 Comment Summary

 A nostalgic tribute to the early, curated web, celebrating its focused sites as "electronic folk art" and highlighting the evident love and care put into their creation.

## 9. [European Commission fines Google €2.95B over abusive ad tech practices](https://news.ycombinator.com/item?id=45140730)

**Total comment counts : 30**

### Summary

 error

### Top 1 Comment Summary

 Despite looming fines, Google won’t exit the EU because it still earns roughly $20 billion in annual net profit after fines, and shareholders wouldn’t tolerate a departure. The company also operates in other protectionist markets—like Korea—where substantial revenue remains possible. China isn’t a counterexample: Google stopped search there when profitability dropped, showing exit decisions hinge on economics rather than pressure alone.

### Top 2 Comment Summary

 European regulators indicate that addressing Google’s conflicts of interest may require Google to divest parts of its services. The European Commission will first hear and assess Google’s proposal before deciding on next steps, signaling a decisive, Europe-led move beyond what the US achieved.

## 10. [How big are our embeddings now and why?](https://news.ycombinator.com/item?id=45101797)

**Total comment counts : 3**

### Summary

 The piece revisits embedding dimensionality, noting a shift from the traditional 200–300 dimensions to larger sizes as models grow. Embeddings are dense, latent vector representations of features (words, images, etc.) used for tasks like search and classification, often built with TF-IDF, PCA/LSA, or neural models (Word2Vec, BERT). Embedding size is a hyperparameter balanced against training throughput, storage, and downstream performance, historically around 300 dims, with BERT using 768. Larger embeddings align with GPU/TPU efficiency and attention-head partitioning.

### Top 1 Comment Summary

 Years ago, the author built a news aggregator using Universal Sentence Encoder (pre-BERT) and found cosine similarity correlated with semantic similarity, aiding clustering. Recently, when testing OpenAI embeddings, they were surprised that cosine similarities were almost the same across texts—often within a 0.2 range—even for unrelated content. They ask why this compression occurs and whether these embeddings are flawed for semantic similarity.

### Top 2 Comment Summary

 The piece argues that Jevons paradox explains why LLMs stay large despite diminishing returns: if we can output 4096D embeddings, why not use all dimensions? Training data and regimen are still bottlenecks, though there’s demand for smaller embedding models for storage and compute. EmbeddingGemma (300m) released recently reportedly beats 4096D Qwen-3 benchmarks at 768D, and its 128D equivalent via MRL outperforms many 768D embedding models.

